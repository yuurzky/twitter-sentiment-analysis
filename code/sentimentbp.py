# -*- coding: utf-8 -*-
"""SentimentBP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q49dyBn1P2D8vcRfHlbQH2CroUqfVe33
"""

!pip install --quiet snscrape
!pip install -U --quiet scikit-learn
!pip install --quiet pandas numpy

import pandas as pd
import random
import re
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
import snscrape.modules.twitter as sntwitter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# Tentukan keyword dan rentang tanggal pencarian
keyword = "blackpink"
start_date = "2020-01-01"
end_date = "2022-12-31"

# Buat query pencarian
query = f"{keyword} since:{start_date} until:{end_date} lang:en"

# Scraping tweet dengan snscrape
tweets = []
for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):
    tweets.append([tweet.date, tweet.rawContent, tweet.user.username, tweet.likeCount, tweet.replyCount, tweet.retweetCount])
    if i >= 10000: # Mengambil data hingga 10.000 tweet
        break

# Random 2000 data dari hasil scraping
if len(tweets) > 2000:
    tweets = random.sample(tweets, 2000)

# Simpan hasil scraping ke dalam dataframe
df = pd.DataFrame(tweets, columns=["Tanggal", "Tweet", "Username", "Likes", "Replies", "Retweets"])

# Menyimpan dataset dengan kolom sentimen baru
df.to_csv("data.csv", index=False)

df = pd.read_csv('data.csv')

df.tail()

df[['Tanggal', 'Tweet']]

# Fungsi untuk melakukan preprocessing
def preprocess_tweet(tweet):
    # Menghapus tanda baca dan karakter khusus
    tweet = re.sub(r'[^\w\s]', '', tweet)

    # Mengubah ke huruf kecil
    tweet = tweet.lower()

    # Menghapus stopwords
    stop_words = set(stopwords.words('english'))
    tokens = nltk.word_tokenize(tweet)
    tokens = [token for token in tokens if token not in stop_words]
    
    # Melakukan lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Menggabungkan kembali token menjadi kalimat
    tweet = ' '.join(tokens)
    return tweet

# Preprocessing pada kolom Tweet dalam DataFrame
df['Tweet'] = df['Tweet'].apply(preprocess_tweet)

# Cek sentimen dengan TextBlob dan tambahkan kolom Sentimen
df['Sentimen'] = df['Tweet'].apply(lambda tweet: TextBlob(tweet).sentiment.polarity)

# Tambahkan kolom Sentimen Label berdasarkan nilai Sentimen
df['Sentimen Label'] = df['Sentimen'].apply(lambda sentimen: "positif" if sentimen > 0 else "negatif" if sentimen < 0 else "netral")

# Menjumlahkan sentimen
jumlah_sentimen = df['Sentimen Label'].value_counts()

# Membuat dataframe baru
df_sentimen = pd.DataFrame(jumlah_sentimen).reset_index()

# Mengubah nama kolom
df_sentimen.columns = ['Sentimen', 'Jumlah']

# Menampilkan hasil
df_sentimen

# Hitung jumlah data untuk setiap Sentimen Label
sentiment_counts = df['Sentimen Label'].value_counts()
neutral_count = sentiment_counts['netral']
positive_count = sentiment_counts['positif']
negative_count = sentiment_counts['negatif']

difference = abs(positive_count - neutral_count)

# Cek apakah ada selisih data dengan proporsi yang diinginkan
print("Selisih data:", difference)

# Tambahkan data dengan sentimen negatif secara otomatis
new_tweets = []
for i in range(difference):
    tweet = sntwitter.TwitterSearchScraper(query).get_items()
    new_tweet = {'Tanggal': tweet.date, 'Tweet': tweet.rawContent, 'Username': tweet.user.username, 'Likes': tweet.likeCount, 'Replies': tweet.replyCount, 'Retweets': tweet.retweetCount, 'Sentimen': TextBlob(preprocess_tweet(tweet.rawContent)).sentiment.polarity, 'Sentimen Label': 'negatif'}
    new_tweets.append(new_tweet)

# # Tambahkan data dengan sentimen positif secara otomatis
# new_tweets = []
# for i in range(difference):
#     tweet = sntwitter.TwitterSearchScraper(query).get_items()
#     new_tweet = {'Tanggal': tweet.date, 'Tweet': tweet.rawContent, 'Username': tweet.user.username, 'Likes': tweet.likeCount, 'Replies': tweet.replyCount, 'Retweets': tweet.retweetCount, 'Sentimen': TextBlob(preprocess_tweet(tweet.rawContent)).sentiment.polarity, 'Sentimen Label': 'positif'}
#     new_tweets.append(new_tweet)

# Gabungkan dataframe yang baru dengan dataframe yang telah ada
new_df = pd.DataFrame(new_tweets)
df = pd.concat([df, new_df], ignore_index=True)

# Menjumlahkan sentimen
jumlah_sentimen = df['Sentimen Label'].value_counts()

# Membuat dataframe baru
df_sentimen = pd.DataFrame(jumlah_sentimen).reset_index()

# Mengubah nama kolom
df_sentimen.columns = ['Sentimen', 'Jumlah']

# Menampilkan hasil
df_sentimen

df.to_csv("data_sentimen.csv", index=False)
df.head()

# Membuat vektor fitur menggunakan CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['Tweet'])

# Menentukan target
y = df['Sentimen Label']

# Membagi data menjadi data latih dan data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Membuat model Naive Bayes Classifier
clf = MultinomialNB()
clf.fit(X_train, y_train)

# Melakukan prediksi pada data uji
y_pred = clf.predict(X_test)

# Menghitung akurasi
acc = accuracy_score(y_test, y_pred)
print("Akurasi:", acc)

# Membuat model SVM
svm_model = SVC(kernel='linear', probability=True)

# Membuat model Naive Bayes
nb_model = MultinomialNB()

# Menggabungkan kedua model dalam voting classifier
voting_model = VotingClassifier(estimators=[('svm', svm_model), ('nb', nb_model)], voting='soft')

# Melatih model pada data training
voting_model.fit(X_train, y_train)

# Melakukan prediksi sentimen pada data testing
y_pred = voting_model.predict(X_test)

# Menghitung akurasi prediksi
accuracy = accuracy_score(y_test, y_pred)
print(f"Akurasi prediksi: {accuracy}")

# Prediksi sentimen pada data baru
new_tweet = "BLACKPINK's Jisoo, Jennie, RosÃ© and Lisa share a mix of memories, nerves, excitement and anticipation as they prepare to close out day 2 of Coachella 2023."
new_tweet = preprocess_tweet(new_tweet)
X_test = vectorizer.transform([new_tweet])
y_pred = voting_model.predict(X_test)

# Cetak hasil prediksi
print("Hasil prediksi sentimen:")
print("Sentimen: ", y_pred[0])